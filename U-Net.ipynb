{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 813639,
          "sourceType": "datasetVersion",
          "datasetId": 427555
        }
      ],
      "dockerImageVersionId": 30699,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "mura01",
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'mura-v11:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F427555%2F813639%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240513%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240513T200523Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D8e5668a8f425067a1312dbc5a0449de71cc8d62ad0df2e73901ff04c21eea29377fc82c17245a2dce9154b714acdaccf157ceb10eb34642881834cc2f00020e6160f80a0038d2dcfc38ef794be9fb91cdcc3da25241ec47844cd35723d3569577b8c6a84110044dbdad241de545a54db22e161a280b6ec1014ab4997a309d6806af52e6b4607686f9df0704580593bd32c0b11b04b515d2839e94ac14d5b5ada4d6249ce59385e10fb3c16de115ee6abc788cf98720b7e6a99f2e45c547d8180a5352c101cab4ef86ed2669e6626cc556ede12ad8f61bb644cb15a72f68196064715ec85570c13e36e4007e2f1a702a431bd71b9eb11755e623d9f8e62d03d29'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "31oh7Uyrj1Zz",
        "outputId": "a8553e3e-290a-42b8-de13-aba3be8543a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading mura-v11, 3374985543 bytes compressed\n",
            "[============================================      ] 2995404800 bytes downloaded"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms, models\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "import os"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-05-13T19:38:33.840709Z",
          "iopub.execute_input": "2024-05-13T19:38:33.84109Z",
          "iopub.status.idle": "2024-05-13T19:38:37.440154Z",
          "shell.execute_reply.started": "2024-05-13T19:38:33.841061Z",
          "shell.execute_reply": "2024-05-13T19:38:37.439136Z"
        },
        "trusted": true,
        "id": "PRsrRZvBj1Z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "directory_path = Path('/kaggle/input/mura-v11/')\n",
        "os.chdir(str(Path(directory_path)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-13T19:38:37.441696Z",
          "iopub.execute_input": "2024-05-13T19:38:37.442137Z",
          "iopub.status.idle": "2024-05-13T19:38:37.480592Z",
          "shell.execute_reply.started": "2024-05-13T19:38:37.44211Z",
          "shell.execute_reply": "2024-05-13T19:38:37.479611Z"
        },
        "trusted": true,
        "id": "quok7-F3j1Z1",
        "outputId": "319a6eb2-a361-4d60-b4ca-85efd613d1c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "cuda\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Path\n",
        "data_path = Path('MURA-v1.1')\n",
        "\n",
        "train_df = pd.read_csv(data_path/'train_image_paths.csv', names=['image_path'])\n",
        "valid_df = pd.read_csv(data_path/'valid_image_paths.csv', names=['image_path'])\n",
        "trainlabel_df = pd.read_csv(data_path/'train_labeled_studies.csv', names=['image_path', 'label'])\n",
        "validlabel_df = pd.read_csv(data_path/'valid_labeled_studies.csv', names=['image_path', 'label'])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-13T19:38:45.679769Z",
          "iopub.execute_input": "2024-05-13T19:38:45.680556Z",
          "iopub.status.idle": "2024-05-13T19:38:45.760189Z",
          "shell.execute_reply.started": "2024-05-13T19:38:45.680524Z",
          "shell.execute_reply": "2024-05-13T19:38:45.759347Z"
        },
        "trusted": true,
        "id": "j-8qYFRzj1Z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.df.iloc[idx, 0]\n",
        "        image = Image.open(image_path).convert('LA')\n",
        "        label = 1 if 'positive' in image_path else 0\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-13T19:38:50.790333Z",
          "iopub.execute_input": "2024-05-13T19:38:50.791438Z",
          "iopub.status.idle": "2024-05-13T19:38:50.797939Z",
          "shell.execute_reply.started": "2024-05-13T19:38:50.791405Z",
          "shell.execute_reply": "2024-05-13T19:38:50.79692Z"
        },
        "trusted": true,
        "id": "aj0j_xEkj1Z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.456], std=[0.224]),\n",
        "])\n",
        "\n",
        "valid_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.456], std=[0.224]),\n",
        "])\n",
        "study_types = ['XR_ELBOW', 'XR_FINGER', 'XR_FOREARM', 'XR_HAND', 'XR_HUMERUS', 'XR_SHOULDER', 'XR_WRIST']\n",
        "study_train_df, study_valid_df, train_dataset, valid_dataset, train_dataloader, valid_dataloader = {}, {}, {}, {}, {}, {}\n",
        "\n",
        "\n",
        "for study_type in study_types:\n",
        "    study_train_df[study_type] = train_df[train_df['image_path'].str.contains(study_type)]\n",
        "    study_valid_df[study_type] = valid_df[valid_df['image_path'].str.contains(study_type)]\n",
        "\n",
        "    train_dataset[study_type] = Dataset(study_train_df[study_type], transform=train_transform)\n",
        "    valid_dataset[study_type] = Dataset(study_valid_df[study_type], transform=valid_transform)\n",
        "\n",
        "    train_dataloader[study_type] = DataLoader(train_dataset[study_type], batch_size=12, shuffle=True)\n",
        "    valid_dataloader[study_type] = DataLoader(valid_dataset[study_type], batch_size=12, shuffle=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-13T19:45:31.137694Z",
          "iopub.execute_input": "2024-05-13T19:45:31.138417Z",
          "iopub.status.idle": "2024-05-13T19:45:31.292657Z",
          "shell.execute_reply.started": "2024-05-13T19:45:31.138382Z",
          "shell.execute_reply": "2024-05-13T19:45:31.291883Z"
        },
        "trusted": true,
        "id": "bZa_w5W8j1Z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for study_type in study_types:\n",
        "    print(study_type, len(train_dataset[study_type]), len(valid_dataset[study_type]))\n",
        "    for batch in train_dataloader[study_type]:\n",
        "        print(batch[0].shape, batch[1].shape)\n",
        "        break"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-13T19:38:56.968922Z",
          "iopub.execute_input": "2024-05-13T19:38:56.969895Z",
          "iopub.status.idle": "2024-05-13T19:38:57.848293Z",
          "shell.execute_reply.started": "2024-05-13T19:38:56.969837Z",
          "shell.execute_reply": "2024-05-13T19:38:57.847248Z"
        },
        "trusted": true,
        "id": "EjacSfhHj1Z2",
        "outputId": "58d6eec0-fcc3-480d-c717-128b137e754a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "XR_ELBOW 4931 465\ntorch.Size([8, 2, 224, 224]) torch.Size([8])\nXR_FINGER 5106 461\ntorch.Size([8, 2, 224, 224]) torch.Size([8])\nXR_FOREARM 1825 301\ntorch.Size([8, 2, 224, 224]) torch.Size([8])\nXR_HAND 5543 460\ntorch.Size([8, 2, 224, 224]) torch.Size([8])\nXR_HUMERUS 1272 288\ntorch.Size([8, 2, 224, 224]) torch.Size([8])\nXR_SHOULDER 8379 563\ntorch.Size([8, 2, 224, 224]) torch.Size([8])\nXR_WRIST 9752 659\ntorch.Size([8, 2, 224, 224]) torch.Size([8])\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(7, 3, figsize=(15, 15))\n",
        "for study_type in study_types:\n",
        "    indices = random.sample(range(len(train_dataset[study_type])), 3)\n",
        "    for i, idx in enumerate(indices):\n",
        "        image_path = study_train_df[study_type].iloc[idx]['image_path']\n",
        "        image = Image.open(image_path).convert('L')\n",
        "        rows = study_types.index(study_type)\n",
        "        cols = i\n",
        "        axs[rows, cols].imshow(image)\n",
        "        axs[rows, cols].axis('off')\n",
        "        axs[rows, cols].set_title(study_type)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "KmnuR13wj1Z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# U-NET Architecture\n",
        "class conv_block(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "        self.sequential = nn.Sequential(\n",
        "            nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_c),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_c),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.sequential(x)\n",
        "\n",
        "class encoder_block(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "        self.conv = conv_block(in_c, out_c)\n",
        "        self.pool = nn.MaxPool2d((2, 2))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.conv(inputs)\n",
        "        p = self.pool(x)\n",
        "        return x, p\n",
        "\n",
        "class decoder_block(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
        "        self.conv = conv_block(out_c+out_c, out_c)\n",
        "\n",
        "    def forward(self, inputs, skip):\n",
        "        x = self.up(inputs)\n",
        "        concat = torch.cat([x, skip], axis=1)\n",
        "        x = self.conv(concat)\n",
        "        return x\n",
        "\n",
        "class build_unet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.enc1 = encoder_block(2, 64)\n",
        "        self.enc2 = encoder_block(64, 128)\n",
        "        self.enc3 = encoder_block(128, 256)\n",
        "        self.enc4 = encoder_block(256, 512)\n",
        "\n",
        "        self.mid = conv_block(512, 1024)\n",
        "\n",
        "        self.dec1 = decoder_block(1024, 512)\n",
        "        self.dec2 = decoder_block(512, 256)\n",
        "        self.dec3 = decoder_block(256, 128)\n",
        "        self.dec4 = decoder_block(128, 64)\n",
        "\n",
        "        self.last = nn.Conv2d(64, 1, kernel_size=1, padding=0)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        enc1, p1 = self.enc1(inputs)\n",
        "        enc2, p2 = self.enc2(p1)\n",
        "        enc3, p3 = self.enc3(p2)\n",
        "        enc4, p4 = self.enc4(p3)\n",
        "\n",
        "        mid = self.mid(p4)\n",
        "\n",
        "        dec1 = self.dec1(mid, enc4)\n",
        "        dec2 = self.dec2(dec1, enc3)\n",
        "        dec3 = self.dec3(dec2, enc2)\n",
        "        dec4 = self.dec4(dec3, enc1)\n",
        "\n",
        "        outputs = self.last(dec4)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-13T19:39:04.157019Z",
          "iopub.execute_input": "2024-05-13T19:39:04.15774Z",
          "iopub.status.idle": "2024-05-13T19:39:04.173059Z",
          "shell.execute_reply.started": "2024-05-13T19:39:04.15771Z",
          "shell.execute_reply": "2024-05-13T19:39:04.172066Z"
        },
        "trusted": true,
        "id": "fOmIghYYj1Z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = build_unet()\n",
        "model2.to(device)\n",
        "\n",
        "from torchinfo import summary"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-13T19:39:17.174308Z",
          "iopub.execute_input": "2024-05-13T19:39:17.175177Z",
          "iopub.status.idle": "2024-05-13T19:39:17.628819Z",
          "shell.execute_reply.started": "2024-05-13T19:39:17.175143Z",
          "shell.execute_reply": "2024-05-13T19:39:17.627902Z"
        },
        "trusted": true,
        "id": "eiT3GGp1j1Z3",
        "outputId": "94237e33-34a5-4ac3-a907-7196fa43e430"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 10,
          "output_type": "execute_result",
          "data": {
            "text/plain": "build_unet(\n  (enc1): encoder_block(\n    (conv): conv_block(\n      (sequential): Sequential(\n        (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n    (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  )\n  (enc2): encoder_block(\n    (conv): conv_block(\n      (sequential): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n    (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  )\n  (enc3): encoder_block(\n    (conv): conv_block(\n      (sequential): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n    (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  )\n  (enc4): encoder_block(\n    (conv): conv_block(\n      (sequential): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n    (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  )\n  (mid): conv_block(\n    (sequential): Sequential(\n      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (dec1): decoder_block(\n    (up): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n    (conv): conv_block(\n      (sequential): Sequential(\n        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (dec2): decoder_block(\n    (up): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n    (conv): conv_block(\n      (sequential): Sequential(\n        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (dec3): decoder_block(\n    (up): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n    (conv): conv_block(\n      (sequential): Sequential(\n        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (dec4): decoder_block(\n    (up): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n    (conv): conv_block(\n      (sequential): Sequential(\n        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (last): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_dataloader, criterion, optimizer):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        output = output[:, 0, 0, 0]\n",
        "        target = target.float()\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return loss.item()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-13T20:03:49.194244Z",
          "iopub.execute_input": "2024-05-13T20:03:49.195158Z",
          "iopub.status.idle": "2024-05-13T20:03:49.202946Z",
          "shell.execute_reply.started": "2024-05-13T20:03:49.195111Z",
          "shell.execute_reply": "2024-05-13T20:03:49.201889Z"
        },
        "trusted": true,
        "id": "68vlrV81j1Z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, val_dataloader, criterion):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_dataloader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            output = output[:, 0, 0, 0]\n",
        "            target = target.float()\n",
        "            loss = criterion(output, target)\n",
        "            val_loss += loss.item()\n",
        "            predicted = (output >= 0.5).float()\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "    val_loss /= len(val_dataloader)\n",
        "    accuracy = (correct / total) * 100\n",
        "    return val_loss, accuracy\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-13T20:03:50.855582Z",
          "iopub.execute_input": "2024-05-13T20:03:50.856215Z",
          "iopub.status.idle": "2024-05-13T20:03:50.863706Z",
          "shell.execute_reply.started": "2024-05-13T20:03:50.856182Z",
          "shell.execute_reply": "2024-05-13T20:03:50.862669Z"
        },
        "trusted": true,
        "id": "JK9TPk_Aj1Z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model2.parameters(), lr=0.001)\n",
        "num_epochs = 50\n",
        "\n",
        "study_types = ['XR_WRIST', 'XR_FINGER', 'XR_FOREARM', 'XR_HAND', 'XR_HUMERUS', 'XR_SHOULDER', 'XR_FOOT', 'XR_ELBOW']\n",
        "study_type = study_types[0]\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model2, train_dataloader[study_type], criterion, optimizer)\n",
        "    validation_loss, accuracy = validate(model2, valid_dataloader[study_type], criterion)\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, '\n",
        "          f'Training Loss: {train_loss:.4f}, '\n",
        "          f'Validation Loss: {validation_loss:.4f}, '\n",
        "          f'Validation Accuracy: {accuracy:.4f}')\n",
        "    print('Training Complete')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-13T20:03:59.157041Z",
          "iopub.execute_input": "2024-05-13T20:03:59.157412Z"
        },
        "trusted": true,
        "id": "zzoNBj6rj1Z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "trusted": true,
        "id": "dtRK15hcj1Z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q98pEzR1j1Z3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}